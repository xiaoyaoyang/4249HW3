{
    "contents" : "\\documentclass[11pt]{article}\n\\lstset{breaklines=true}\n<<setup, include=FALSE>>=\n    render_listings()\n@\n\n\\setlength{\\topmargin}{-.5in}\n\\setlength{\\textheight}{9in}\n\\setlength{\\oddsidemargin}{.125in}\n\\setlength{\\textwidth}{6.25in}\n\n\n\\usepackage{graphicx}\n\\usepackage{amsfonts,amssymb,amsmath}\n\\DeclareGraphicsExtensions{.png,.jpg}\n\n\\title{Statistical Machine Learning HW3}\n\\author{Xiaoyao Yang\\\\ Columbia University}\n\\date{\\today}\n\n\\begin{document}\n\\maketitle\n\n\\section{$\\l_q$ regression}\n\\label{sec:p1}\n\n\\begin{itemize}\n\\item[(1)]\nThe $\\l_{0.5}$ cost function encourage sparse estimates, while cost function with $\\l_{4}$ is not.\nThe reason behind this is that cost function with $\\l_{0.5}$ has sharpe boundry while $\\l_{4}$ has more soomthy boundry. Since $\\l_{0.5}$ has this unique property, the best solution for the minimization problem would always located at point where some $\\beta_i$ are equal to zero. On the contrast, the solution with $\\l_{4}$ norm cost function are hardly located at point where any of $\\beta_i$ is zero, especially for high dimension case.\n\n\\item[(2)]\nFor left plot with $\\l_{0.5}$ norm cost function, point \\(x_3\\) will reach the minimum point. For right plot with $\\l_{4}$ norm cost function, point \\(x_4\\) has the smallest cost function. The reason for the left figure is obvious, since \\(x_3\\) is the only point satisfied the $\\l_{0.5}$ constraint. As for the right figure, all of \\(x_3\\), \\(x_4\\) and \\(x_5\\) have same mean square cost function, but point \\(x_3\\) has the smallest $\\l_p$ norm constraint(it is basically the distance from point to origin). Thus, point \\(x_3\\) is our answer.\n\\end{itemize}\n\n\\section{Combining kernels}\n\\label{sec:p2}\n\n\n\\begin{itemize}\n\\item[ 1)]\nBy Mercer's Theorem,\n\n\\begin{equation}\n\\begin{split}\nak_1(x,x^{'})&=\\displaystyle a\\sum_{j=1}^{\\infty}\\lambda_j\\phi_j(x)\\phi_j(x^{'})\\\\\n&=\\displaystyle\\sum_{j=1}^{\\infty}\\lambda_j\\sqrt{a}\\phi_j(x)\\sqrt{a}\\phi_j(x^{'})\n\\end{split}\n\\end{equation}\n\nWe can define \\(\\phi(x_{new})=(\\sqrt{\\lambda_1}\\phi_1(x_{new}),...,\\sqrt{\\lambda_2}\\phi_D(x_{new}))\\)\nthen \\(ak_1(x,x^{'})\\) is a kernel.\n\n\\item[ 2)]\n\\begin{equation}\n\\begin{split}\nk_1(x,x^{'})k_2(x,x^{'})&=\\displaystyle \\sum_{j=1}^{\\infty}\\lambda_j\\phi_j(x)\\phi_j(x^{'})\\displaystyle \\sum_{i=1}^{\\infty}\\rho_i\\varphi_i(x)\\varphi_i(x^{'})\\\\\n&=\\displaystyle\\sum_{j=1}^{\\infty}\\displaystyle\\sum_{i=1}^{\\infty}\\lambda_j\\rho_i\\phi_j(x)\\phi_j(x^{'})\\varphi_i(x)\\varphi_i(x^{'})\\\\\n&=\\displaystyle\\sum_{ij}\\lambda_j \\rho_i \\{\\phi_j(x) \\varphi_i(x)\\} \\{\\phi_j(x^{'}) \\varphi_i(x^{'})\\}\n\\end{split}\n\\end{equation}\n\nHere both of \\(\\phi_i(x)\\) and \\(\\phi_j(x)\\) are function \\( R^{d} \\mapsto R\\). Thus  \\(\\phi_i(x)\\phi_j(x)\\) is also a function \\( R^{d} \\mapsto R\\). If we define \\(\\lambda_{new}^{k}=\\lambda_i\\rho_j\\) for every i,j. we can see that \\(k_1(x,x^{'})k_2(x,x^{'})\\) is also satidfy Mercer's Theorem.\n\n\\item[ 3)]\nWe can derive this result based on the result above. We already knew that if \\(k_1(x,x^{'})\\) is kernel, \\(k_1(x,x^{'})k_1(x,x^{'})\\) is a kernel. And by using mathematical induction, we can get that \\(k_1(x,x^{'})^{3}\\) is a kernel...and so on. Anyway, we will end up with for any positive p, \\(k_1(x,x^{'})^{p}\\) is a kernel\n\n\\end{itemize}\n\n\\section{Adaboost}\n\\label{sec:p3}\n\n<<ada1,cache=TRUE,size='small',tidy=TRUE,results='markup'>>=\nrequire(e1071)\nset.seed(1024)\ndat<-read.table('uspsdata.txt',header=F)\nclass<-read.table('uspscl.txt',header=F)\nX <- dat\ny <- class\n\n# given predictor matrix X, with label y, and weight of data. return weak learner pars(j,theta,m)\ntrain <- function(X,w,y)\n{\n    if(class(w)!='matrix'){\n        w <- as.matrix(w)\n    }\n#     DecisionStump <- function(X,w,y) \n#     decision stump\n#     w must be 1 column\n#     find best theta for each dim\n    theta <- vector()\n    best.cost <- vector()\n    Compare <- 10\n    for(d in 1:dim(X)[2]){\n        \n        for(n in 1:dim(X)[1]){\n            yhat <- 2*(X[,d]>X[n,d])-1\n            cost.temp <- t(w)%*%(y!=yhat)/sum(w)\n            if (cost.temp<Compare) {\n                Compare <- cost.temp\n                best.theta <- X[n,d] \n                best.j<-d\n                best.n<-n    \n            }\n        }\n    }\n    return(list(j=best.j,theta=best.theta,m=1))\n}\n\n# Given data set dat, and learner pars. Return predicted label.\nclassify <- function(dat,pars)\n{\n#     X must have same rcol with training data\n    rcol <- pars[['j']]\n    theta <- pars$theta\n    yhat <- 2*(dat[,rcol]>theta)-1\n    return(label=yhat)\n}\n\n# Given data set X, all weak learner weight alpha, and all parameters of weak learners\n# Return boosting classifier c_hat\nagg_class <- function(X,alpha,allPars)\n{\n#     allPars is list of lists\n    classifierT <- sapply(allPars,FUN=classify,dat=X)\n    classifier <- t(classifierT)\n    c_hat <- sign(alpha%*%classifier)\n    return(c_hat)\n}\n\n###Some implementations\nw=rep(1/200,200)\npars <- train(X=X,w=w,y=y)\npars\n\n#label is the predicted value for the first 100 obs using\n#weak learner \"pars\"\nlabel <- classify(dat=dat,pars=pars)\nlabel\n\n#error rate with decision stumps\nmean(label!=y)\n@\n\n\n<<r-2,cache=TRUE,size='small',tidy=TRUE>>=\n#Given training data, return boosting weight and all weak learner pars\nAdaboost <- function(dat.train,y.train,B=10)\n{\n    #implement boosting\n    if (!is.data.frame(y.train)) {y.train<-data.frame(y.train=y.train)}\n    allPars <- matrix(list())\n    n<-dim(y.train)[1]\n    if (n!=dim(dat.train)[1]){stop('data and label must have same length')}\n        \n    w=rep(1/n,n)\n    alpha <- vector()\n        for (b in 1:B){\n            pars <- train(X=dat.train,w,y.train)\n            yhat <- classify(dat.train,pars)\n            error <- t(w)%*%(yhat!=y.train)/sum(w)\n            vote <- as.numeric(log((1-error)/error))\n            w <- w*exp(vote*(yhat!=y.train))\n            alpha[b] <- vote\n            allPars[[b]] <- pars\n        }\n        return(list(alpha=alpha,allPars=allPars))\n}\n\nK=5\ndat<-X\nb=1\n#given original data set,with class label y, calculate K-fold CV error and train error using adaboosting\n#with B weak learner\nCalculateError <- function(dat,y,K,B)\n{\n    folds <- data.frame(Fold = sample(rep(1:K, length.out = NROW(dat))), \n                        Row = 1:NROW(dat))\n    train.error <- 0\n    cv.error <- 0\n    for (f in 1:max(folds$Fold)) {\n        theRows <- folds$Row[folds$Fold == f]\n        fit <- Adaboost(dat.train=dat[-theRows,],y.train=y[-theRows,],B=B)\n        yhat <- agg_class(X=dat[theRows,],alpha=fit$alpha,allPars=fit$allPars)\n        yhat.train <- agg_class(X=dat[-theRows,],alpha=fit$alpha,allPars=fit$allPars)\n        theCost <- mean(yhat!=y[theRows,])\n        theCost.train <- mean(yhat.train!=y[-theRows,])\n        cv.error <- cv.error + theCost * (length(theRows)/NROW(dat))\n        train.error <- train.error + theCost.train*(length(theRows)/NROW(dat))\n    }\n    return(list(cv.error=cv.error,train.error=train.error))\n}\n\n##some implementations with Adaboost and agg_class\nfit <- Adaboost(dat.train=dat,y.train=y,B=3)\nfit$alpha\nfit$allPars\n\nc_hat <- as.vector(agg_class(X=dat,alpha=fit$alpha,allPars=fit$allPars))\nc_hat\n#error with three weak learner\nmean(c_hat!=y)\n@\n\n\\large {We can find out that by boosting using 3 weak learner, the training error for entire dataset is 0.155, which is lower that the error obtained by single weak learner}\n\n\\normalsize\n<<r-3,cache=TRUE,dev='png',size='small',tidy=TRUE>>=\n    #plot \nadaplot <- function()\n{\n    B=20\n    cv.error <- vector()\n    train.error <- vector()\n    for(b in 1:B)\n        {\n            fit <- CalculateError(dat=dat,y=y,K=5,B=b)\n            cv.error[b] <- fit$cv.error\n            train.error[b] <- fit$train.error\n        }\n    dataplot<-data.frame(B=rep(1:22,length.out=44),Error=c(train.error,cv.error),\n                     ErrorType=rep(c('TrainingError','CrossValidationError'),each=22))\nrequire(ggplot2)\ng <- ggplot(data=dataplot,aes(B,Error,group=ErrorType,shape=ErrorType)) + \n    geom_line(mapping=aes(linetype=ErrorType)) +\n    geom_point()\ng + labs(x='Number of weak wearner',y='Error') +\n    ggtitle(\n        label='Change of training and cross validation\n            \\n error with iteration of boosting')\n}\n@\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{errplot.jpg}\n  \\label{fig:adaplot}\n\\end{figure}\n\n\\large From Figure above we can see that both the training error and cv error decrease with increase of weak learner. And the training error is always higher than cv error, which makes perfect sense.\n\\normalsize\n\n\\end{document}\n",
    "created" : 1396926750844.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "771989228",
    "id" : "348B9341",
    "lastKnownWriteTime" : 1396489067,
    "path" : "~/Google Drive/Schoolwork/2014Spring/W4400/HW3/4400HW3.rnw",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "sweave"
}